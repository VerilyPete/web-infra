# .github/workflows/deploy-or-update-via-tailscale.yml
# This workflow is used to deploy a new instance to a private subnet without the use of a private runner
# It deploys a new instance via OCI CLI if one does not exist, or updates an existing instance if one does exist
# Cloud-init is used to bootstrap tailscale, at which point Github Actions takes over and finishes the deployment

name: Deploy or Update via Tailscale

on:
  workflow_dispatch:
    inputs:
      deploy_type:
        description: 'Deployment type'
        required: true
        default: 'update'
        type: choice
        options:
          - update
          - fresh_deploy
      hostname:
        description: 'Target hostname'
        required: true
        type: choice
        options:
          - webserver-staging
          - webserver-prod


env:
  OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
  OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
  OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
  OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
  OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set hostname
        id: hostname
        run: |
          echo "hostname=${{ github.event.inputs.hostname }}" >> $GITHUB_OUTPUT

      - name: Fresh deploy - Check for existing instance & terminate if found
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 5
        continue-on-error: true
        id: find_existing_instance
        with:
          command: 'compute instance list --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --lifecycle-state RUNNING'
          query: 'data[?\"display-name\" == `${{ github.event.inputs.hostname }}`].{id: id, name: \"display-name\"} | [0]'
          silent: false

      - name: Terminate existing instance if found
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        id: terminate_existing
        run: |
          INSTANCE_DATA="${{ steps.find_existing_instance.outputs.output }}"
          if [ "$INSTANCE_DATA" != "null" ] && [ -n "$INSTANCE_DATA" ] && [ "$INSTANCE_DATA" != '""' ]; then
            INSTANCE_ID=$(echo "$INSTANCE_DATA" | jq -r '.id // empty')
            if [ -n "$INSTANCE_ID" ]; then
              echo "Found existing instance $INSTANCE_ID, terminating..."
              oci compute instance terminate --instance-id "$INSTANCE_ID" --force --wait-for-state TERMINATED
              echo "‚úÖ Existing instance terminated"
            fi
          else
            echo "No existing instance found with name ${{ github.event.inputs.hostname }}"
          fi

      - name: Deploy new instance (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 15
        id: create_instance
        with:
          command: 'compute instance launch --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}" --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --shape "VM.Standard.A1.Flex" --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}" --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}" --subnet-id "${{ secrets.OCI_PRIVATE_SUBNET }}" --user-data-file tailscale-cloud-init.yml --display-name "${{ steps.hostname.outputs.hostname }}" --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"${{ steps.hostname.outputs.hostname }}\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}" --wait-for-state RUNNING --max-wait-seconds 900'
          silent: false

      - name: Parse instance ID (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        id: parse_instance_id
        run: |
          # Remove outer quotes and unescape the JSON string
          CLEAN_JSON=$(echo '${{ steps.create_instance.outputs.output }}' | sed 's/^"//; s/"$//; s/\\"/"/g')
          INSTANCE_ID=$(echo "$CLEAN_JSON" | jq -r '.data.id')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        if: github.event.inputs.deploy_type == 'update' || github.event.inputs.deploy_type == 'fresh_deploy'
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Wait for Tailscale connectivity (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        timeout-minutes: 10
        run: |
          TARGET_HOSTNAME="${{ steps.hostname.outputs.hostname }}"
          echo "Waiting for Tailscale connectivity to $TARGET_HOSTNAME..."
          
          MAX_ATTEMPTS=30
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if timeout 5 ssh -o ConnectTimeout=3 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$TARGET_HOSTNAME "echo 'Tailscale Ready'" 2>/dev/null; then
              echo "‚úÖ Tailscale connection successful to $TARGET_HOSTNAME (attempt $i)"
              break
            fi
            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "‚ùå Tailscale connection failed after maximum attempts"
              echo "üí° Check Tailscale admin console and instance cloud-init logs"
              exit 1
            fi
            echo "Tailscale attempt $i/$MAX_ATTEMPTS failed, waiting 20 seconds..."
            sleep 20
          done

      - name: Setup infrastructure
        if: github.event.inputs.deploy_type == 'update' || github.event.inputs.deploy_type == 'fresh_deploy'
        timeout-minutes: 8
        run: |
          # Use the selected hostname
          TARGET_HOSTNAME="${{ github.event.inputs.hostname }}"
          echo "Target instance: $TARGET_HOSTNAME"
          
          # Determine Cloudflare tunnel token based on hostname
          if [ "$TARGET_HOSTNAME" = "webserver-prod" ]; then
            CLOUDFLARE_TOKEN="${{ secrets.CLOUDFLARE_PROD_TUNNEL_TOKEN }}"
            echo "Using production Cloudflare tunnel token"
          else
            CLOUDFLARE_TOKEN="${{ secrets.CLOUDFLARE_STAGING_TUNNEL_TOKEN }}"
            echo "Using staging Cloudflare tunnel token"
          fi
          
          # Quick SSH connectivity check via Tailscale
          echo "Checking Tailscale SSH access to $TARGET_HOSTNAME..."
          SSH_READY=false
          MAX_ATTEMPTS=6
          
          # Ensure XDG_RUNTIME_DIR is set for user systemd services
          export XDG_RUNTIME_DIR="/run/user/$(ssh -o ConnectTimeout=3 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$TARGET_HOSTNAME "id -u" 2>/dev/null)"
          if [ -z "$XDG_RUNTIME_DIR" ] || [ "$XDG_RUNTIME_DIR" = "/run/user/" ]; then
            echo "‚ö†Ô∏è  Could not determine XDG_RUNTIME_DIR, using default"
            export XDG_RUNTIME_DIR="/run/user/1000"
          fi
          
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if timeout 5 ssh -o ConnectTimeout=3 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$TARGET_HOSTNAME "echo 'SSH Ready'" 2>/dev/null; then
              echo "‚úÖ SSH connection successful via Tailscale (attempt $i)"
              SSH_READY=true
              break
            fi
            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "‚ùå SSH connection failed after maximum attempts"
              exit 1
            fi
            echo "SSH attempt $i/$MAX_ATTEMPTS failed, waiting 10 seconds..."
            sleep 10
          done
          
          # Setup infrastructure
          echo "Setting up infrastructure on $TARGET_HOSTNAME..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=15 opc@$TARGET_HOSTNAME << EOF
          set -e
          
          echo "Setting up system configuration..."
          
          # setup subuid/subgid
          if ! grep -q "^opc:" /etc/subuid 2>/dev/null; then echo "opc:100000:65536" | sudo tee -a /etc/subuid; fi
          if ! grep -q "^opc:" /etc/subgid 2>/dev/null; then echo "opc:100000:65536" | sudo tee -a /etc/subgid; fi
          
          # Setup cron job for podman cleanup (if not already done)
          if ! crontab -l 2>/dev/null | grep -q "podman-cleanup.sh"; then
            (crontab -l 2>/dev/null; echo "0 2 * * * /usr/local/bin/podman-cleanup.sh >> /var/log/podman-cleanup.log 2>&1") | crontab -
          fi
          
          # Ensure user systemd service is enabled and running (already done in cloud-init)
          # Just verify the user systemd is working
          systemctl --user daemon-reload
          
          # Create cleanup script (if not exists)
          if [ ! -f "/usr/local/bin/podman-cleanup.sh" ]; then
            sudo tee /usr/local/bin/podman-cleanup.sh > /dev/null << 'CLEANUP_EOF'
          #!/bin/bash
          for user in $(getent passwd | grep -E '/home|/var/lib' | cut -d: -f1); do
            if id "$user" &>/dev/null; then
              sudo -u "$user" podman container prune -f 2>/dev/null || true
              sudo -u "$user" podman image prune -af --filter "until=24h" 2>/dev/null || true
              sudo -u "$user" podman volume prune -f 2>/dev/null || true
              sudo -u "$user" podman system prune -af 2>/dev/null || true
            fi
          done
          podman container prune -f; podman image prune -af --filter "until=24h"; podman volume prune -f; podman system prune -af
          buildah rmi --prune; journalctl --vacuum-time=7d; journalctl --vacuum-size=500M
          CLEANUP_EOF
            sudo chmod +x /usr/local/bin/podman-cleanup.sh
          fi
          
          echo "‚úÖ System configuration complete"
          
          echo "Setting up container infrastructure..."
          
          # Clone/update repository
          if [ ! -d "/home/opc/webserver" ]; then
            git clone https://github.com/VerilyPete/webserver.git /home/opc/webserver
          else
            cd /home/opc/webserver && git pull origin main
          fi

          echo "Setting up environment variables..."
          cd ~/webserver
          
          # Create .env file
          cat > .env << ENV_EOF
          HOSTNAME=${{ github.event.inputs.hostname }}
          TAILSCALE_AUTH_KEY=${{ secrets.TAILSCALE_AUTH_KEY }}
          CLOUDFLARE_TUNNEL_TOKEN=${CLOUDFLARE_TOKEN}
          FORMSPREE_ENDPOINT=${{ secrets.FORMSPREE_ENDPOINT }}
          APP_PORT=8081
          APP_ENV=production
          ENV_EOF
          
          chmod 600 .env
          
          # Create systemd service files
          mkdir -p ~/.config/systemd/user
          
          cat > ~/.config/systemd/user/webserver-pod.service << 'SERVICE_EOF'
          [Unit]
          Description=Web Infrastructure Pod
          Wants=network-online.target
          After=network-online.target
          RequiresMountsFor=%t/containers
          
          [Service]
          Type=oneshot
          RemainAfterExit=yes
          Restart=on-failure
          RestartSec=10
          TimeoutStopSec=70
          WorkingDirectory=%h/webserver
          Environment=PODMAN_SYSTEMD_UNIT=%n
          ExecStart=/usr/local/bin/start-web-pod.sh
          ExecStop=/usr/bin/podman pod stop webserver-pod
          ExecStopPost=/usr/bin/podman pod rm -f webserver-pod
          
          [Install]
          WantedBy=default.target
          SERVICE_EOF
          
          # Create start script
          sudo tee /usr/local/bin/start-web-pod.sh > /dev/null << 'START_EOF'
          #!/bin/bash
          set -e

          # Ensure we're in the right directory
          cd /home/opc/webserver || { echo "ERROR: Cannot change to /home/opc/webserver"; exit 1; }

          # Load environment with better error handling
          if [ ! -f ".env" ]; then
            echo "ERROR: .env file not found in $(pwd)"
            exit 1
          fi
          source .env

          # Setup
          CONFIG_DIR="/home/opc/webserver/config"
          mkdir -p "$CONFIG_DIR"
          NGINX_CONFIG_FILE="$CONFIG_DIR/nginx.conf"

          echo "Starting web infrastructure setup..."

          # Create pod and cleanup existing containers (but don't touch tailscale)
          echo "Creating pod and cleaning up existing containers..."
          podman pod create --name webserver-pod --publish 8081:8081 --replace 2>/dev/null || true
          podman stop web-pod web cloudflared 2>/dev/null || true
          podman rm web-pod web cloudflared 2>/dev/null || true
          
          # Verify tailscale container is still running
          if ! podman ps --format "{{.Names}}" | grep -q "^tailscale$"; then
            echo "‚ùå Tailscale container is not running. This will break connectivity!"
            exit 1
          else
            echo "‚úÖ Tailscale container is running"
          fi

          # Pull image and generate config
          echo "Pulling latest image and generating nginx config..."
          podman pull ghcr.io/verilypete/webserver:latest
          podman run --rm --env FORMSPREE_ENDPOINT="$FORMSPREE_ENDPOINT" \
            ghcr.io/verilypete/webserver:latest \
          sh -c 'sed "s|__FORMSPREE_ENDPOINT__|$FORMSPREE_ENDPOINT|g" /etc/nginx/nginx.conf' > "$NGINX_CONFIG_FILE"

          # Fix permissions and SELinux context so nginx can read the file
          chmod 644 "$NGINX_CONFIG_FILE"
          # Set SELinux context for container access (if SELinux is enabled)
          if command -v selinuxenabled >/dev/null 2>&1 && selinuxenabled; then
            chcon -Rt container_file_t "$NGINX_CONFIG_FILE" 2>/dev/null || true
          fi

          # Start core containers
          echo "Starting core containers..."
          podman run -d --name web-pod --pod webserver-pod --restart unless-stopped k8s.gcr.io/pause:3.9
          podman run -d --name web --pod webserver-pod --restart unless-stopped \
            --mount type=bind,source="$NGINX_CONFIG_FILE",target=/etc/nginx/nginx.conf,ro \
            --pull=always ghcr.io/verilypete/webserver:main-111f3e3

          # Start optional services
          if [ -n "$CLOUDFLARE_TUNNEL_TOKEN" ] && [ "$CLOUDFLARE_TUNNEL_TOKEN" != "your-tunnel-token-here" ]; then
            echo "Starting Cloudflare tunnel..."
            podman run -d --name cloudflared --pod webserver-pod --restart unless-stopped \
              --env TUNNEL_TOKEN="$CLOUDFLARE_TUNNEL_TOKEN" --pull=always docker.io/cloudflare/cloudflared:latest tunnel --no-autoupdate run
          fi

          echo "‚úÖ Web server started on http://localhost:8081"
          START_EOF
          
          sudo chmod +x /usr/local/bin/start-web-pod.sh

          # Start the service
          export XDG_RUNTIME_DIR="/run/user/$(id -u)"
          sleep 2
          systemctl --user daemon-reload
          systemctl --user enable webserver-pod.service
          systemctl --user restart webserver-pod.service
          
          # Wait a moment and check service status
          sleep 5
          if systemctl --user is-active --quiet webserver-pod.service; then
            echo "‚úÖ Service started successfully"
          else
            echo "‚ùå Service failed to start. Checking logs..."
            systemctl --user status webserver-pod.service --no-pager -l
            journalctl --user -u webserver-pod.service --no-pager -l --since "1 minute ago"
            exit 1
          fi
          
          echo "‚úÖ Infrastructure setup completed successfully"
          EOF

      - name: Display new instance info (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 2
        continue-on-error: true
        with:
          command: 'compute instance get --instance-id "${{ steps.parse_instance_id.outputs.instance_id }}"'
          query: 'data.{id: id, name: \"display-name\", state: \"lifecycle-state\", shape: shape, region: region}'

      - name: Verify deployment
        run: |
          echo "‚úÖ Deployment completed successfully!"
          echo ""
          if [ "${{ github.event.inputs.deploy_type }}" = "fresh_deploy" ]; then
            echo "üÜï New instance created:"
            echo "   Instance ID: ${{ steps.parse_instance_id.outputs.instance_id }}"
            echo "   Tailscale Hostname: ${{ github.event.inputs.hostname }}"
          else
            echo "üîÑ Existing instance updated:"
            echo "   Tailscale Hostname: ${{ github.event.inputs.hostname }}"
          fi
          echo ""
          echo "üîç Check your Tailscale admin console for the device"
          echo "üåê Web server accessible at http://${{ github.event.inputs.hostname }}:8081 (via Tailscale)"
          echo "üìä Monitor with: ssh opc@${{ github.event.inputs.hostname }} 'podman pod ps && podman ps'"

  cleanup-old-instances:
    runs-on: ubuntu-latest
    timeout-minutes: 8
    needs: deploy
    if: github.event.inputs.deploy_type == 'fresh_deploy'
    
    env:
      OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
      OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
      OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
      OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
      OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}
    
    steps:
      - name: List old instances for manual cleanup
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 3
        with:
          command: 'compute instance list --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --lifecycle-state RUNNING'
          query: 'data[?contains(\"display-name\", `webserver`) && \"display-name\" != `${{ github.event.inputs.hostname }}`].{Name:\"display-name\", ID:id, Created:\"time-created\"}'
          silent: false

      - name: Cleanup instructions
        run: |
          echo ""
          echo "üßπ Old instances listed above may need cleanup"
          echo "üí° To terminate old instances:"
          echo "   Use the OCI Console or run:"
          echo "   oci compute instance terminate --instance-id <INSTANCE_ID> --force"